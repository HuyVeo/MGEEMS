{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Xâu chung dài nhất\n",
   "id": "d7355492a7dec201"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-27T02:48:23.659072Z",
     "start_time": "2026-01-27T02:48:14.491682Z"
    }
   },
   "source": [
    "from underthesea import word_tokenize\n",
    "input_file = \"D:\\Khóa_Luận_Tốt_Nghiệp\\MGEEMS\\Test\\Data\\gloss.txt\"\n",
    "output_file = \"D:\\Khóa_Luận_Tốt_Nghiệp\\MGEEMS\\Test\\Similarity\\Gloss\\SimLCS.txt\"\n",
    "\n",
    "def tokenize_vi(text):\n",
    "    return word_tokenize(text.lower(), format=\"text\").split()\n",
    "\n",
    "def lcs_length(s1, s2):\n",
    "    n, m = len(s1), len(s2)\n",
    "    L = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                L[i][j] = L[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "    return L[n][m]\n",
    "def lcs_similarity(tokens1, tokens2):\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    return 4.0 * lcs_length(tokens1, tokens2) / max(len(tokens1), len(tokens2))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\K'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\K'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\K'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\K'\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_17744\\657934483.py:2: SyntaxWarning: invalid escape sequence '\\K'\n",
      "  input_file = \"D:\\Khóa_Luận_Tốt_Nghiệp\\MGEEMS\\Test\\Data\\gloss.txt\"\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_17744\\657934483.py:3: SyntaxWarning: invalid escape sequence '\\K'\n",
      "  output_file = \"D:\\Khóa_Luận_Tốt_Nghiệp\\MGEEMS\\Test\\Similarity\\Gloss\\SimLCS.txt\"\n",
      "C:\\Users\\ADMIN\\anaconda3\\envs\\Semantic-Concept-Similarity\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(input_file, encoding=\"utf-8\") as f:\n",
    "    glosses = [line.strip() for line in f if line.strip()]\n",
    "assert len(glosses) % 2 == 0, \"Số dòng gloss phải là số chẵn!\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for i in range(0, len(glosses), 2):\n",
    "        g1 = glosses[i]\n",
    "        g2 = glosses[i + 1]\n",
    "        tokens1 = tokenize_vi(g1)\n",
    "        tokens2 = tokenize_vi(g2)\n",
    "        sim = lcs_similarity(tokens1, tokens2)\n",
    "        # ghi ra file\n",
    "        fout.write(f\"{i//2}\\t{sim:.3f}\\n\")"
   ],
   "id": "b838445f31330763"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
